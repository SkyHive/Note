#  CEPH分布式存储

#### 对象存储

简单认为是分布式的`key-value存储`

对象存储将元数据独立了出来，控制节点叫元数据服务器（服务器+对象存储管理软件），主要负责存储对象的属性（主要是对象的元数据被打散存放到了那几台分布式服务器中的信息），而其他负责存储数据的分布式服务器叫做OSD，主要负责存储文件的数据部分。

##### 存储方式对比

* 对象存储（键值数据库）：接口简单，一个对象我们可以看成一个文件，只能全写全读，通常以大文件为主，要求足够的IO带宽
* 块存储（硬盘）：IO特点与传统的硬盘一致，一个硬盘应该是能面向通用需求的，即能应付大文件读写，也能处理好小文件读写。由于硬盘的特点是容量大，热点明显，因此块存储主要可以应付热点问题。另外，块存储要求的延迟是最低的
* 文件存储（文件系统）：支持文件存储的接口的系统设计跟传统本地文件系统如Ext4这种的特点和难点是一致的，它比块存储更具丰富的接口。

#### CEPH（分布式文件系统）

利用一个分布式集群来提供对象，块和文件存储的统一存储平台。CEPH是一种基于软件的解决方案，并在商品硬件上运行。

##### CEPH架构

![1553926803794](C:\Users\zhongyi.yang\AppData\Roaming\Typora\typora-user-images\1553926803794.png)

CEPH 生态系统可以大致分为四部分

* 客户端（数据用户）

* 元数据服务器（缓存和同步分布式元数据）

* 对象存储集群（将数据和元数据作为对象存储，执行其他相关职能）

* 集群监视器（执行监视功能）

  

##### CEPH设计

* server端：提供一个存储集群`ceph storage cluster`，核心组件`RADOS`
* client端：ceph提供三种存储方式：
  * CEPH Filesystem
  * CEPH Block Device
  * CEPH Object Storage



##### RADOS

CEPH存储集群的基础，所有服务都从其中读取和写入数据。

包括两类守护进程：

* 对象存储守护进程（OSD）存储为对象
* 监视器（Monitor）维护者集群运行Map的主拷贝

**一个集群可以包含数千个存储节点，至少需要两个OSD才能数据复制**



##### CEPH Filesystem

CephFS 是一个兼容 POSIX 协议的文件系统，可以用类似挂载 NFS 的方式挂载至多台机器使用。

基本存储组件

CephFS 是搭建在 Rados 上的一个 Client，Rados 的主要组件有两个：

* OSD（Object Storage Device）是真正存储的组件
* Mon（Monitor）维护整个集群各个组件的状态，所有的 Client 访问 Ceph 时都是先与 Mon 交互，得到响应资源的 Map，再执行具体的操作

FS 组件

为了使用 CephFS，还需要一个额外的组件 MDS（Meta Data Server），主要用于管理 CephFS 的 Meta 信息，需要注意的是 MDS 并不直接存存储 Meta Data。



##### CEPH Block Device

块设备将信息存储在固定大小的块中，每个块都有自己的地址。数据块的大小通常在512字节到32768字节之间。块

设备的基本特征是每个块都能独立于其他块而读写。磁盘是最常见的块设备。



##### CEPH Object Storage

对象存储系统是综合了 NAS 和 SAN 的优点，同时具有 SAN 的高速直通直接访问和 NAS 的数据共享等优势，提供了高可靠性、跨平台性以及安全的数据共享的存储体系结构